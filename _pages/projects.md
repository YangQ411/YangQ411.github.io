---
title: "Projects"
permalink: /projects/
author_profile: true 
use_math: true
---

Here are some of my selected projects:

## Large Language Models (LLMs)

## Hybrid Fine-Tuning and Parallelism for Llama3 Model
- Fine-tuned the Llama3.2-1B model using Low-Rank Adaptation (LoRA) to reduce parameter overhead.  
- Implemented and compared mixed-precision training (FP32, FP16, BF16) to optimize speed and memory efficiency.  
- Applied distributed training strategies using Data Parallelism (DDP) and Fully Sharded Data Parallelism (FSDP) across multiple GPUs to improve scalability.  
- Conducted experiments on Northeastern’s Explorer HPC Cluster, utilizing multi-GPU resources and Slurm job scheduling for large-scale training and benchmarking.  

[GitHub Repo](https://github.com/YangQ411/llama-lora-study)

---

## Personalized Chatbot with RLHF
- Designing a domain-specific chatbox capable of emulating the style of a renowned economist to address targeted economic questions.  
- Leveraging a pretrained LLaMA-3 model as the base system and extending it with reinforcement learning methods (PPO/DPO) for alignment.  
- Planning to integrate preference-based reward modeling to refine responses toward domain-specific quality and consistency.  
- Expected outcome: demonstrate the effectiveness of RLHF/DPO for style emulation and domain adaptation, highlighting practical applications of aligned LLMs.

[GitHub Repo](https://github.com/YangQ411/llama-lora-study)

---

## Survey and Benchmarking of LLM Evaluation Methods
- Fine-tuned a Llama-3 model with LoRA on the OpenOrca dataset to develop a customized instruction-tuned model for evaluation experiments.  
- Analyzed model outputs along QA accuracy and instruction-following dimensions via token-level generation analysis.  
- Implemented LLM-based automatic evaluation using GPT-4 as the judge, assessing the fine-tuned model on MT-Bench and a custom dataset to measure performance reliably.  
- Compared traditional metrics (BLEU, ROUGE, Perplexity) with LLM-based judgments, identifying the key advantages and limitations of each paradigm to guide more robust evaluation strategies.

[GitHub Repo](https://github.com/YangQ411/llama-lora-study)

---

## Physics Research (Past Work)

## Non-singular String Cosmology with Matter Sources via all Order \( \alpha^\prime \) Corrections
- Conducted background research on classical singularities in cosmology. 
- Studied string-theoretic corrections in non-singular cosmology via \( \alpha^\prime \) expansion and derived kinematic equations describing cosmic evolution. 
- Solved equations using Wolfram Mathematica and visualized solution trends for matter-driven cosmological models. 

[Paper Draft](#)

---

## Derive Quasi-topological Gravity from String Theory in Cosmological Background
- Made a literature review of quasi-topological gravity and the process of deriving Lovelock’s theory of gravity from string theory.  
- Calculated the curvature tensor using a self-designed program with python.  
- Derived quasi-topological gravity from string theory in the cosmological background.  

[Paper Draft](#)